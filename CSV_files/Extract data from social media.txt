# Import required libraries
import pandas as pd
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import nltk
from nltk.corpus import stopwords

# Download NLTK stopwords and punkt tokenizer
nltk.download('stopwords')
nltk.download('punkt')

# Load the dataset
data = pd.read_csv("C:/Users/91959/Desktop/p4 Retail marketing/reviews.csv")

# Step 1: Display basic information about the dataset
print("Dataset Info:")
print(data.info())  # Provides details on column names, types, and missing values

# Step 2: Clean and Preprocess the Data
def clean_text(text):
    """
    Cleans the input text by:
    - Converting to lowercase
    - Removing newlines
    - Removing special characters
    """
    text = str(text).lower()  # Convert to lowercase
    text = text.replace('\n', ' ')  # Remove newlines
    text = ''.join(char for char in text if char.isalnum() or char.isspace())  # Remove special characters
    return text

# Apply cleaning function to the 'Text' column
data['Cleaned_Review'] = data['Text'].apply(clean_text)

# Step 3: Perform Sentiment Analysis
def analyze_sentiment(text):
    """
    Analyzes the sentiment polarity of a given text using TextBlob.
    Returns a polarity score between -1 (negative) and 1 (positive).
    """
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Sample 1000 reviews randomly for analysis to improve performance
data = data.sample(1000, random_state=42)

# Apply sentiment analysis function
data['Sentiment'] = data['Cleaned_Review'].apply(analyze_sentiment)

# Categorize sentiment based on polarity
def sentiment_category(score):
    """
    Categorizes sentiment into Positive, Negative, or Neutral based on polarity score.
    """
    if score > 0:
        return 'Positive'
    elif score < 0:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment categorization function
data['Sentiment_Category'] = data['Sentiment'].apply(sentiment_category)

# Step 4: Analyze Trends or Metrics
# Count the occurrences of each sentiment category
sentiment_counts = data['Sentiment_Category'].value_counts()
print("\nSentiment Counts:")
print(sentiment_counts)

# Step 5: Visualize the Findings
# Bar plot for sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')
plt.title('Sentiment Distribution of Reviews', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.show()

# Word Frequency Analysis
# Define stop words and filter them out from the cleaned reviews
stop_words = set(stopwords.words('english'))
all_words = ' '.join(data['Cleaned_Review']).split()
filtered_words = [word for word in all_words if word not in stop_words]

# Count the frequency of words
word_counts = Counter(filtered_words)
most_common_words = word_counts.most_common(10)

# Visualize Word Frequencies
words, counts = zip(*most_common_words)
plt.figure(figsize=(10, 6))
sns.barplot(x=list(counts), y=list(words), palette='coolwarm', orient='h')
plt.title('Top 10 Most Common Words in Reviews', fontsize=16)
plt.xlabel('Frequency', fontsize=14)
plt.ylabel('Words', fontsize=14)
plt.show()

# Step 6: Save cleaned data for further use
data.to_csv('cleaned_reviews.csv', index=False)
print("\nAnalysis complete. Cleaned data saved to 'cleaned_reviews.csv'.")
